{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b267a05",
   "metadata": {},
   "source": [
    "# Text Encoding process \n",
    "\n",
    "This notebook is to work on the text encoding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c336f6",
   "metadata": {},
   "source": [
    "Use the same example as LLMs from scratch to work on text encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "147dde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "if not Path(\"data/the-verdict.txt\").exists():\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    if not Path(\"data\").exists():\n",
    "        Path(\"data\").mkdir()\n",
    "    file_path = \"data/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39bbe2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e5c5be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'simple', 'example', '.', \"Let's\", 'see', 'how', 'it', 'works!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "simple_example = \"This is a simple example. Let's see how it works!\"\n",
    "\n",
    "# Split by space, comma, or period, keeping the delimiters\n",
    "tokens = re.split(r'([,.]|\\s)', simple_example)\n",
    "# Remove empty tokens\n",
    "tokens = [token for token in tokens if token.strip()]\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69348e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius',\n",
       " '--',\n",
       " 'though',\n",
       " 'a',\n",
       " 'good',\n",
       " 'fellow',\n",
       " 'enough',\n",
       " '--',\n",
       " 'so',\n",
       " 'it',\n",
       " 'was']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed_text = [token.strip() for token in preprocessed_text if token.strip()]\n",
    "\n",
    "preprocessed_text[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c5e2c",
   "metadata": {},
   "source": [
    "## Vanilla text processing \n",
    "\n",
    "Now we have our processed text, we can work on encoding it.\n",
    "\n",
    "Let's create a simple vocabulary mapping each unique token to an integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "071ff7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: idx for idx, token in enumerate(sorted(set(preprocessed_text)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbb5eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n",
      "Encoded: [97, 584, 156, 7]\n",
      "Decoded: This is an.\n"
     ]
    }
   ],
   "source": [
    "class TextEncoderV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = {idx: token for token, idx in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        text = [self.inv_vocab[idx] for idx in indices]\n",
    "        return re.sub(r' ([,.:;?_!\"()\\'])', r'\\1', ' '.join(text))\n",
    "    \n",
    "encoder = TextEncoderV1(vocab)\n",
    "print(\"Vocabulary size:\", len(encoder.vocab))\n",
    "print(\"Encoded:\", encoder.encode(\"This is an.\"))\n",
    "print(\"Decoded:\", encoder.decode(encoder.encode(\"This is an.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12770682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <unk> token for unknown words and end-of-sequence token <eos>\n",
    "new_vocbab = vocab.copy()\n",
    "new_vocbab[\"<unk>\"] = len(new_vocbab)\n",
    "new_vocbab[\"<eos>\"] = len(new_vocbab)\n",
    "\n",
    "class TextEncoderV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = {idx: token for token, idx in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        tokens = [token if token in self.vocab else \"<unk>\" for token in tokens\n",
    "                   ]\n",
    "        tokens.append(\"<eos>\")\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        text = [self.inv_vocab[idx] for idx in indices if idx != self.vocab[\"<eos>\"]]\n",
    "        return re.sub(r' ([,.:;?_!\"()\\'])', r'\\1', ' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f3ff049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded with V2: [97, 584, 156, 1130, 7, 1131]\n",
      "Decoded with V2: This is an <unk>.\n"
     ]
    }
   ],
   "source": [
    "encoder_v2 = TextEncoderV2(new_vocbab)\n",
    "print(\"Encoded with V2:\", encoder_v2.encode(\"This is an unknownword.\"))\n",
    "print(\"Decoded with V2:\", encoder_v2.decode(encoder_v2.encode(\"This is an unknownword.\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85275ab",
   "metadata": {},
   "source": [
    "Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "877f8e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "from tiktoken._educational import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "words = \"Akwirw ier\"\n",
    "\n",
    "tokens = tokenizer.encode(words)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "decoded_words = tokenizer.decode(tokens)\n",
    "\n",
    "print(decoded_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde1d04",
   "metadata": {},
   "source": [
    "Implementing rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbde661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "Total number of tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"Total number of tokens:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7708cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_seq = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f77a3c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: [290, 4920, 2241, 287]\n",
      "Target tokens:     [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_window = 4 \n",
    "\n",
    "x = enc_seq[:context_window]\n",
    "y = enc_seq[1:context_window + 1]\n",
    "\n",
    "print(f\"Input tokens: {x}\")\n",
    "print(f\"Target tokens:     {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53118c",
   "metadata": {},
   "source": [
    "Pytorch dataset implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85362953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence 0: tensor([  40.,  367., 2885., 1464.])\n",
      "Target sequence 0:     tensor([ 367., 2885., 1464., 1807.])\n",
      "\n",
      "Input sequence 1: tensor([1807., 3619.,  402.,  271.])\n",
      "Target sequence 1:     tensor([ 3619.,   402.,   271., 10899.])\n",
      "\n",
      "Input sequence 2: tensor([10899.,  2138.,   257.,  7026.])\n",
      "Target sequence 2:     tensor([ 2138.,   257.,  7026., 15632.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride) -> None:\n",
    "        self.input_ids = []\n",
    "        self.output_ids = []\n",
    "\n",
    "        tokens = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(tokens) - max_length, stride):\n",
    "            input_seq = tokens[i:i + max_length]\n",
    "            target_seq = tokens[i + 1:i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(input_seq)\n",
    "            self.output_ids.append(target_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.input_ids[index]), torch.Tensor(self.output_ids[index])\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, max_length=4, stride=4)\n",
    "\n",
    "# Exercise: Implementing rolling windows\n",
    "for i in range(3):\n",
    "    input_seq, target_seq = dataset[i]\n",
    "    print(f\"Input sequence {i}: {input_seq}\")\n",
    "    print(f\"Target sequence {i}:     {target_seq}\")\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe85b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[3.9300e+02, 2.8537e+04, 2.0140e+03, 1.9800e+02],\n",
      "        [7.6400e+02, 7.6400e+02, 7.6400e+02, 1.3750e+03],\n",
      "        [3.1900e+02, 4.6500e+02, 6.3600e+02, 1.1000e+01],\n",
      "        [2.9200e+02, 3.1400e+02, 1.0226e+04, 2.6200e+02],\n",
      "        [4.1851e+04, 5.1500e+02, 5.0200e+02, 9.9100e+02],\n",
      "        [8.0400e+02, 3.7900e+02, 3.2600e+02, 1.5170e+03],\n",
      "        [1.1000e+01, 2.5700e+02, 2.2791e+04, 2.7800e+02],\n",
      "        [1.0000e+00, 3.5700e+02, 4.0000e+01, 9.5770e+03]]), tensor([[2.8537e+04, 2.0140e+03, 1.9800e+02, 1.9800e+02],\n",
      "        [7.6400e+02, 7.6400e+02, 1.3750e+03, 1.9080e+03],\n",
      "        [4.6500e+02, 6.3600e+02, 1.1000e+01, 2.5700e+02],\n",
      "        [3.1400e+02, 1.0226e+04, 2.6200e+02, 1.1820e+03],\n",
      "        [5.1500e+02, 5.0200e+02, 9.9100e+02, 5.1700e+02],\n",
      "        [3.7900e+02, 3.2600e+02, 1.5170e+03, 4.3800e+02],\n",
      "        [2.5700e+02, 2.2791e+04, 2.7800e+02, 2.8600e+02],\n",
      "        [3.5700e+02, 4.0000e+01, 9.5770e+03, 4.5440e+03]])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bef1ef",
   "metadata": {},
   "source": [
    "Look at the positionnal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59db6dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "output_dim = 256 \n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "example = next(iter(dataloader))[0].long()\n",
    "example = token_embedding_layer(example)\n",
    "\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bb96408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2333, -1.3073,  0.8137,  ...,  1.0845, -1.0451,  0.4195],\n",
       "        [ 0.2661,  1.7172,  0.0182,  ...,  0.8971, -0.9394, -0.8210],\n",
       "        [ 1.0633, -0.4465, -2.1395,  ...,  1.6644, -0.4589, -3.0300],\n",
       "        [-0.0652,  0.5328, -0.1788,  ...,  0.5779, -1.1191, -0.6639]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embedding_layer = nn.Embedding(context_window, output_dim)\n",
    "\n",
    "pos_embedding = positional_embedding_layer(torch.arange(context_window))\n",
    "pos_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a792bb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embedding = example + pos_embedding\n",
    "input_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b192a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
