# LLMs from Papers

This repository aims to understand and implement what makes Large Language Models (LLMs) work, by reading and implementing the original papers. 

My main source are going to be the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and the [Attention is All You Need paper](https://arxiv.org/abs/1706.03762). And also the book [Hands-On Large Language Models](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/). 

My mains tools are Python and PyTorch.

## Current Status 
- [x] Tokenization (using tiktoken)
- [x] Attention mechanism from scratch
- [x] GPT-2 model architecture from scratch
- [ ] Training a small GPT-2 model on a small dataset
- [ ] Implement training loop from scratch
